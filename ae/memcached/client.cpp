#include <arpa/inet.h>
#include <netinet/in.h>
#include <sys/socket.h>
#include <unistd.h>

#include <algorithm>
#include <atomic>
#include <cassert>
#include <chrono>
#include <cmath>
#include <cstdio>
#include <iostream>
#include <cstring>
#include <mutex>
#include <numeric>
#include <random>
#include <thread>
#include <vector>

#include "common.hpp"
#include "utils.hpp"

#ifdef __cplusplus
extern "C" {
#endif
#include <sei/crc.h>
#ifdef __cplusplus
}
#endif

namespace monitor {

// monitor the throughput and latency of events
// log: the file descriptor to output summary
// num_ops: total number of operations for all threads
// n_threads: number of threads executing the ops
// task: task name of the evaluation
// cnts: # of operations executed on each thread
// latency: the latency for each operation by counting with rdtsc()
// report: report on stderr for most recent throughput, with last_scnt and
// last_rdtsc value
struct evaluation {
    static constexpr int max_n_threads = 256;
    evaluation(FILE *log, uint64_t num_ops, int n_threads, std::string task);
    ~evaluation();
    FILE *log;
    uint64_t num_ops;
    int n_threads;
    std::string task;
    struct alignas(64) Cnt {
        uint64_t c;
    };
    std::vector<uint64_t> latency;
    Cnt cnts[max_n_threads];
    std::vector<std::pair<std::chrono::steady_clock::time_point, uint64_t>>
        records;
    std::vector<uint64_t> scnts;
    void report();
};

evaluation::evaluation(FILE *log, uint64_t num_ops, int n_threads,
                       std::string task)
    : log(log), num_ops(num_ops), n_threads(n_threads), task(task) {
    latency.resize(num_ops);
    records.emplace_back(std::chrono::steady_clock::now(), 0);
    for (int i = 0; i < max_n_threads; ++i) cnts[i].c = 0;
}

evaluation::~evaluation() {
    if (num_ops == 0) return;

    uint64_t n_phases = std::min<uint64_t>(num_ops, 8LU);
    uint64_t l = 0, r = num_ops;
    if (n_phases > 1) {
        l = num_ops / n_phases;
        r = num_ops * (n_phases - 1) / n_phases;
    }
    if (r < l) std::swap(l, r);
    if (r <= l) {
        l = 0;
        r = num_ops;
    }
    const uint64_t span = r - l;
    if (span == 0) {
        l = 0;
        r = num_ops;
    }

    std::sort(latency.begin() + l, latency.begin() + r);
    const uint64_t n = r - l;
    const uint64_t p90 = nanosecond(0, latency[l + uint64_t(n * .9)]);
    const uint64_t p95 = nanosecond(0, latency[l + uint64_t(n * .95)]);
    const uint64_t p99 = nanosecond(0, latency[l + uint64_t(n * .99)]);
    const uint64_t avg =
        nanosecond(0, std::accumulate(latency.begin() + l, latency.begin() + r,
                                      0ULL)) /
        (n ? n : 1);
    auto period = std::chrono::duration_cast<std::chrono::microseconds>(
        std::chrono::steady_clock::now() - records[0].first);
    const long long total_us = std::max<long long>(period.count(), 1);
    fprintf(stderr, "Finished task %s. Time: %lld us; Throughput: %f/s.\n",
            task.c_str(), total_us, num_ops * 1e6 / total_us);
    uint64_t put = static_cast<uint64_t>(num_ops) * 1000000ULL /
                   static_cast<uint64_t>(total_us);
    if (records.size() >= 2) {
        const uint64_t last = records.size() - 1;
        const uint64_t max_r = last - 1;  // we use (r + 1) below
        uint64_t lrec = 0, rrec = max_r;
        if (n_phases > 1 && max_r > 0) {
            lrec = max_r / n_phases;
            rrec = max_r * (n_phases - 1) / n_phases;
        }
        if (rrec < lrec) std::swap(lrec, rrec);
        if (lrec > max_r) lrec = max_r;
        if (rrec > max_r) rrec = max_r;
        period = std::chrono::duration_cast<std::chrono::microseconds>(
            records[rrec + 1].first - records[lrec].first);
        const long long window_us = std::max<long long>(period.count(), 1);
        // NOTE: Use put as the estimated throughput
        put = (records[rrec + 1].second - records[lrec].second) * 1000000LL /
              window_us;
    }
    fprintf(stderr, "Estimated (operation) throughput: %lu/s\n", put);
    fprintf(log, "%s put %lu avg %lu p90 %lu p95 %lu p99 %lu\n", task.c_str(),
            put, avg, p90, p95, p99);
}

void evaluation::report() {
    static std::mutex lock;
    lock.lock();
    uint64_t cnt = 0;
    for (int i = 0; i < n_threads; ++i) cnt += cnts[i].c;
    if (cnt > records.back().second + 16384) {  // minimum print interval
        auto now = std::chrono::steady_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::microseconds>(
                            now - records.back().first)
                            .count();
        fprintf(stderr, "Instant throughput: %f/s\n",
                (cnt - records.back().second) * 1e6 / duration);
    }
    records.emplace_back(std::chrono::steady_clock::now(), cnt);
    lock.unlock();
}

}  // namespace monitor

static std::string ip, output_file;
static uint32_t port, ngroups, nsets, ngets, nclients, rps;
static double read_pct = -1.0;
static uint64_t nupdates = 0;

static inline void write_all(int fd, const char *buf, size_t len) {
    size_t written = 0;
    while (written < len) {
        ssize_t ret = write(fd, buf + written, len - written);
        assert(ret > 0);
        written += ret;
    }
}

class MemcpyMonad {
public:
    MemcpyMonad(void *dst) : dst_(reinterpret_cast<char *>(dst)), offset_(0) {}
    MemcpyMonad &Copy(const void *src, size_t len) {
        memcpy(dst_ + offset_, src, len);
        offset_ += len;
        return *this;
    }
    size_t Offset() { return offset_; }

private:
    char *dst_;
    size_t offset_;
};

static inline size_t prepare_setcmd(char *dst, const char *key,
                                    const char *val) {
    MemcpyMonad m(dst);
    m.Copy("set ", strlen("set "))
        .Copy(key, KEY_LEN)
        .Copy(" ", 1)
        .Copy(val, VAL_LEN)
        .Copy(kCrlf, strlen(kCrlf));
    return m.Offset();
}

static inline size_t prepare_getcmd(char *dst, const char *key) {
    MemcpyMonad m(dst);
    m.Copy("get ", strlen("get "))
        .Copy(key, KEY_LEN)
        .Copy(kCrlf, strlen(kCrlf));
    return m.Offset();
}

static inline size_t prepare_delcmd(char *dst, const char *key) {
    MemcpyMonad m(dst);
    m.Copy("del ", strlen("del "))
        .Copy(key, KEY_LEN)
        .Copy(kCrlf, strlen(kCrlf));
    return m.Offset();
}

static inline int parse_getret(char *rx_buf, size_t rx_len, char *value,
                               size_t value_buf_len, size_t *value_len) {
    size_t prefix_len = strlen(kRetVals[kValue]);
    if (strncmp(rx_buf, kRetVals[kValue], prefix_len) != 0) {
        return -1;
    }
    if (rx_buf[rx_len - 2] != '\r' || rx_buf[rx_len - 1] != '\n') {
        return -1;
    }
    const char *p = rx_buf + prefix_len;
    size_t vlen = rx_len - prefix_len - 2;
    assert(vlen <= value_buf_len);
    memcpy(value, p, vlen);
    *value_len = vlen;
    return 0;
}

inline std::chrono::steady_clock::time_point microtime(void) {
    auto start = std::chrono::steady_clock::now();
    return start;
}

inline uint64_t microtime_diff(
    const std::chrono::steady_clock::time_point &start,
    const std::chrono::steady_clock::time_point &end) {
    std::chrono::microseconds diff =
        std::chrono::duration_cast<std::chrono::microseconds>(end - start);
    return diff.count();
}

inline void random_string(char *data, uint32_t len, std::mt19937 *rng) {
    std::uniform_int_distribution<int> distribution('a', 'z' + 1);
    for (uint32_t i = 0; i < len; i++) {
        data[i] = char(distribution(*rng));
    }
}

static constexpr char kKeyAlphabet[] =
    "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
static constexpr uint64_t kKeyAlphabetSize = sizeof(kKeyAlphabet) - 1;
static constexpr uint64_t kKeyPermuteMul = 11400714819323198485ULL;

static inline uint64_t keyspace() {
    uint64_t space = 1;
    for (size_t i = 0; i < KEY_LEN; ++i) {
        space *= kKeyAlphabetSize;
    }
    return space;
}

static inline void encode_key(char *dst, uint64_t idx, uint64_t space) {
    uint64_t v = (idx * kKeyPermuteMul) % space;
    for (size_t i = 0; i < KEY_LEN; ++i) {
        dst[KEY_LEN - 1 - i] = kKeyAlphabet[v % kKeyAlphabetSize];
        v /= kKeyAlphabetSize;
    }
}

const double kZipfParamS = 1.16;  // to match zipfian distribution
const uint32_t kNumPrints = 32;
const uint32_t kMaxNumThreads = 128;
const int kBufferSize = 1024;
static constexpr size_t kCrcPrefixMax = 16;

static inline size_t prepend_crc_prefix(char *dst, size_t prefix_capacity,
                                        const char *payload,
                                        size_t payload_len) {
    const uint32_t crc = crc_compute(payload, payload_len);
    const int prefix_len = snprintf(dst, prefix_capacity, "%u#", crc);
    assert(prefix_len > 0);
    assert(static_cast<size_t>(prefix_len) < prefix_capacity);
    memmove(dst + prefix_len, payload, payload_len);
    return static_cast<size_t>(prefix_len) + payload_len;
}

FILE *logger;
uint32_t kNumThreads, kNumOpsPerThread;

struct Key {
    char data[KEY_LEN];
};
struct Value {
    char data[VAL_LEN];
};

std::unique_ptr<std::mt19937> rngs[kMaxNumThreads];
std::vector<Key> all_keys;
std::vector<Value> all_vals;
std::vector<uint32_t> zipf_key_indices;
uint32_t kNumKVPairs;

void init_array() {
    kNumThreads = nclients;
    kNumOpsPerThread = ngets;
    kNumKVPairs = nsets;
    all_keys.resize(kNumKVPairs);
    all_vals.resize(kNumKVPairs);
    zipf_key_indices.resize(kNumOpsPerThread * kNumThreads);
}

void init_rng() {
    for (uint32_t i = 0; i < kMaxNumThreads; i++) {
        rngs[i] = std::make_unique<std::mt19937>((i + 1) * port);
        for (uint32_t t = 0; t < 10000; ++t) (*rngs[i])();
    }
}

int connect_server(int group_id) {
    struct sockaddr_in server_addr;
    int fd;

    if ((fd = socket(AF_INET, SOCK_STREAM, 0)) < 0) {
        printf(" create socket error!\n ");
        exit(1);
    }

    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    inet_aton(ip.c_str(), &server_addr.sin_addr);
    server_addr.sin_port = htons(port + group_id);

    if (connect(fd, (struct sockaddr *)&server_addr, sizeof(server_addr)) < 0) {
        assert(false);
    }

    my_usleep(1000);
    return fd;
}

void prepare_key() {
    fprintf(stderr, "Prepare keys...\n");
    const uint64_t space = keyspace();
    if (uint64_t(kNumKVPairs) > space) {
        fprintf(
            stderr,
            "Too many keys (%u) for KEY_LEN=%zu (max=%lu). Reduce nsets or increase KEY_LEN.\n",
            kNumKVPairs,
            size_t(KEY_LEN),
            space);
        exit(1);
    }
    std::vector<std::thread> threads;
    auto start = microtime();
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads.emplace_back([i, space]() {
            uint32_t start_idx = i * kNumKVPairs / kNumThreads;
            uint32_t end_idx =
                std::min((i + 1) * kNumKVPairs / kNumThreads, kNumKVPairs);
            for (uint32_t k = start_idx; k < end_idx; ++k) {
                encode_key(all_keys[k].data, uint64_t(k), space);
            }
        });
    }
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads[i].join();
    }
    threads.clear();
    auto end = microtime();
    fprintf(stderr, "Prepare %d kv pairs, time: %ld us, avg throughput: %f/s\n",
            kNumKVPairs, microtime_diff(start, end),
            kNumKVPairs * 1000000.0 / microtime_diff(start, end));
}

void prepare_zipf_index() {
    fprintf(stderr, "Generate zipfian indices...\n");
    zipf_table_distribution<> zipf(kNumKVPairs, kZipfParamS);
    std::vector<std::thread> threads;
    auto start = microtime();
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads.emplace_back([i, &zipf]() {
            for (uint32_t k = 0; k < kNumOpsPerThread; ++k) {
                zipf_key_indices[k * kNumThreads + i] = zipf(*rngs[i]);
            }
        });
    }
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads[i].join();
    }
    threads.clear();
    auto end = microtime();
    fprintf(
        stderr,
        "Generate %ld zipf key indices, time: %ld us, avg throughput: %f/s\n",
        (uint64_t)kNumOpsPerThread * kNumThreads, microtime_diff(start, end),
        (uint64_t)kNumOpsPerThread * kNumThreads * 1000000.0 /
            microtime_diff(start, end));
}

template <RetType ret_type>
void run_set() {
    std::string task = ret_type == kCreated ? "SET" : "UPDATE";
    fprintf(stderr, "%s (nthreads=%d) start running...\n", task.c_str(),
            kNumThreads);
    std::vector<std::thread> threads;
    monitor::evaluation monitor(logger, kNumKVPairs, kNumThreads, task);
    // upper bound speed = 100K per thread
    uint64_t rps_per_thread = 100000;
    if constexpr (ret_type != kCreated) {
        if (rps > 0) rps_per_thread = rps * ngroups / kNumThreads;
    }
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads.emplace_back([i, &monitor, rps_per_thread]() {
            constexpr uint64_t BNS = 1e6;
            std::exponential_distribution<double> sampler(rps_per_thread / 1e9);
            std::mt19937 rng(1235467 + i);
            uint64_t t_start = rdtsc();
            double t_dur = 0;
            int fd = connect_server(i % ngroups);
            assert(fd >= 0);
            std::vector<char> tx_buf(kBufferSize);
            std::vector<char> rx_buf(kBufferSize);
            for (uint32_t k = i; k < kNumKVPairs; k += kNumThreads) {
                t_dur += sampler(rng);
                uint64_t p = rdtsc(), t_offset = 0;
                uint64_t t_now = nanosecond(t_start, p);
                if (t_now + BNS < t_dur) {
                    my_nsleep(t_dur - t_now - (BNS / 2));
                } else if (t_dur + BNS < t_now) {
                    t_offset = t_now - t_dur - (BNS / 2);
                }
                auto &key = all_keys[k];
                auto &val = all_vals[k];
                random_string(val.data, VAL_LEN, rngs[i].get());
                char *payload = tx_buf.data() + kCrcPrefixMax;
                size_t payload_len = prepare_setcmd(payload, key.data, val.data);
                size_t len = prepend_crc_prefix(tx_buf.data(), kCrcPrefixMax,
                                                payload, payload_len);

                uint64_t timestamp = rdtsc();
                write_all(fd, tx_buf.data(), len);
                size_t rx_len = read(fd, rx_buf.data(), kBufferSize);
                monitor.latency[k] = nanosecond(p, rdtsc()) + t_offset;

                assert(rx_len > 0);
                if (strncmp(rx_buf.data(), kRetVals[ret_type],
                            sizeof(kRetVals[ret_type]) - 1) != 0) {
                    printf("Set error: key %s\n",
                           std::string(key.data, KEY_LEN).c_str());
                }
                monitor.cnts[i].c++;

                uint64_t completed = (uint64_t)(k + kNumThreads) * kNumPrints;
                if (completed % kNumKVPairs < kNumThreads * kNumPrints) {
                    completed /= kNumKVPairs;
                    if (completed % kNumThreads == i) {
                        monitor.report();
                    }
                }
            }
            close(fd);
        });
    }
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads[i].join();
    }
    threads.clear();
}

void run_update() {
    fprintf(stderr, "UPDATE (nthreads=%d) start running...\n", kNumThreads);
    std::vector<std::thread> threads;
    monitor::evaluation monitor(logger, nupdates, kNumThreads, "UPDATE");
    // upper bound speed = 100K per thread
    uint64_t rps_per_thread = 100000;
    if (rps > 0) rps_per_thread = rps * ngroups / kNumThreads;
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads.emplace_back([i, &monitor, rps_per_thread]() {
            constexpr uint64_t BNS = 1e6;
            std::exponential_distribution<double> sampler(rps_per_thread / 1e9);
            std::mt19937 rng(1235467 + i);
            uint64_t t_start = rdtsc();
            double t_dur = 0;

            const uint64_t nops = nupdates;
            if (nops <= i) return;
            const uint64_t nops_i = (nops - i + kNumThreads - 1) / kNumThreads;

            // Each key index belongs to exactly one thread (key % kNumThreads).
            // This avoids concurrent updates to the same key, which would break
            // the post-update GET validation (all_vals must match server state).
            const uint64_t nkeys_i =
                (kNumKVPairs > i)
                    ? (uint64_t(kNumKVPairs - i) + kNumThreads - 1) /
                          kNumThreads
                    : 0;
            assert(nkeys_i > 0);

            int fd = connect_server(i % ngroups);
            assert(fd >= 0);
            std::vector<char> tx_buf(kBufferSize);
            std::vector<char> rx_buf(kBufferSize);
            for (uint64_t j = 0; j < nops_i; ++j) {
                const uint64_t op = j * kNumThreads + i;
                assert(op < nops);

                t_dur += sampler(rng);
                uint64_t p = rdtsc(), t_offset = 0;
                uint64_t t_now = nanosecond(t_start, p);
                if (t_now + BNS < t_dur) {
                    my_nsleep(t_dur - t_now - (BNS / 2));
                } else if (t_dur + BNS < t_now) {
                    t_offset = t_now - t_dur - (BNS / 2);
                }

                const uint32_t key_idx =
                    i + uint32_t((j % nkeys_i) * kNumThreads);
                auto &key = all_keys[key_idx];
                auto &val = all_vals[key_idx];
                random_string(val.data, VAL_LEN, rngs[i].get());
                char *payload = tx_buf.data() + kCrcPrefixMax;
                size_t payload_len = prepare_setcmd(payload, key.data, val.data);
                size_t len = prepend_crc_prefix(tx_buf.data(), kCrcPrefixMax,
                                                payload, payload_len);

                write_all(fd, tx_buf.data(), len);
                size_t rx_len = read(fd, rx_buf.data(), kBufferSize);
                monitor.latency[op] = nanosecond(p, rdtsc()) + t_offset;
                assert(rx_len > 0);
                if (strncmp(rx_buf.data(), kRetVals[kStored],
                            sizeof(kRetVals[kStored]) - 1) != 0) {
                    printf("Update error: key %s, ret %s\n",
                           std::string(key.data, KEY_LEN).c_str(),
                           std::string(rx_buf.data(), rx_len).c_str());
                    assert(false);
                }
                monitor.cnts[i].c++;

                uint64_t completed = (op + 1) * kNumPrints;
                if (completed % nops < kNumPrints) {
                    completed /= nops;
                    if (completed % kNumThreads == i) {
                        monitor.report();
                    }
                }
            }
            close(fd);
        });
    }
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads[i].join();
    }
    threads.clear();
}

void run_get() {
    prepare_zipf_index();
    fprintf(stderr, "GET (nthreads=%d) start running...\n", kNumThreads);
    std::vector<std::thread> threads;
    monitor::evaluation monitor(logger, kNumOpsPerThread * kNumThreads,
                                kNumThreads, "GET");
    uint64_t rps_per_thread = 100000;
    if (rps > 0) rps_per_thread = rps * ngroups / kNumThreads;
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads.emplace_back([i, &monitor, rps_per_thread]() {
            constexpr uint64_t BNS = 1e6;
            std::exponential_distribution<double> sampler(rps_per_thread / 1e9);
            std::mt19937 rng(1235467 + i);
            uint64_t t_start = rdtsc();
            double t_dur = 0;
            int fd = connect_server(i % ngroups);
            assert(fd >= 0);
            Value val;
            std::vector<char> tx_buf(kBufferSize);
            std::vector<char> rx_buf(kBufferSize);
            for (uint32_t k = 0; k < kNumOpsPerThread; ++k) {
                t_dur += sampler(rng);
                uint64_t p = rdtsc(), t_offset = 0;
                uint64_t t_now = nanosecond(t_start, p);
                if (t_now + BNS < t_dur) {
                    my_nsleep(t_dur - t_now - (BNS / 2));
                } else if (t_dur + BNS < t_now) {
                    t_offset = t_now - t_dur - (BNS / 2);
                }
                auto &key = all_keys[zipf_key_indices[k * kNumThreads + i]];
                auto &val = all_vals[zipf_key_indices[k * kNumThreads + i]];
                char buf[VAL_LEN];
                size_t val_len;
                char *payload = tx_buf.data() + kCrcPrefixMax;
                size_t payload_len = prepare_getcmd(payload, key.data);
                size_t len = prepend_crc_prefix(tx_buf.data(), kCrcPrefixMax,
                                                payload, payload_len);
                write_all(fd, tx_buf.data(), len);
                size_t rx_len = read(fd, rx_buf.data(), kBufferSize);
                monitor.latency[k * kNumThreads + i] = nanosecond(p, rdtsc()) + t_offset;
                assert(rx_len > 0);
                int r =
                    parse_getret(rx_buf.data(), rx_len, buf, VAL_LEN, &val_len);
                if (r != 0) {
                    printf("Get error: key %s\n",
                           std::string(key.data, KEY_LEN).c_str());
                } else {
                    assert(val_len == VAL_LEN);
                    if (memcmp(val.data, buf, VAL_LEN)) {
                        printf("Get error: key %s, val %s %s\n",
                               std::string(key.data, KEY_LEN).c_str(),
                               std::string(val.data, VAL_LEN).c_str(),
                               std::string(buf, VAL_LEN).c_str());
                        assert(false);
                    }
                }
                monitor.cnts[i].c++;

                uint64_t completed = (uint64_t)(k + 1) * kNumPrints;
                if (completed % kNumOpsPerThread < kNumPrints) {
                    completed /= kNumOpsPerThread;
                    if (completed % kNumThreads == i) {
                        monitor.report();
                    }
                }
            }
            close(fd);
        });
    }
    for (uint32_t i = 0; i < kNumThreads; ++i) {
        threads[i].join();
    }
    for (uint32_t i = 0; i < ngroups; ++i) {
        int fd = connect_server(i);
        write_all(fd, "quit\n", 5);
    }
    threads.clear();
}

int main(int argc, char **argv) {
    if (argc > 10 || argc <= 1) {
        fprintf(stderr,
                "Usage: %s <ip> <port> <log_file> <ngroups> <nclients> <nsets> "
                "<ngets> <rps> [read_pct]\n",
                argv[0]);
        fprintf(stderr,
                "Default values: ip=127.0.0.1, port=6379, log_file=client.log, "
                "ngroups=3, nclients=32, nsets=3<<24, ngets=1<<19, rps=0, "
                "read_pct=(disabled)\n");
        return 1;
    }
    ip = argc >= 2 ? argv[1] : "127.0.0.1";
    port = argc >= 3 ? atoi(argv[2]) : 6379;
    output_file = argc >= 4 ? argv[3] : "client.log";
    ngroups = argc >= 5 ? atoi(argv[4]) : 3;
    nclients = argc >= 6 ? atoi(argv[5]) : 32;
    nsets = argc >= 7 ? ngroups << atoi(argv[6]) : 3 << 24;
    ngets = argc >= 8 ? 1 << atoi(argv[7]) : 1 << 19;
    rps = argc >= 9 ? atoi(argv[8]) : 0;
    if (argc >= 10) {
        read_pct = atof(argv[9]);
        if (read_pct <= 1.0) {
            read_pct *= 100.0;
        }
        if (!(read_pct > 0.0 && read_pct <= 100.0)) {
            fprintf(stderr, "Invalid read_pct: %f (expected 0<read_pct<=100)\n",
                    read_pct);
            return 1;
        }
    }

    const uint64_t ngets_total = uint64_t(ngets) * uint64_t(nclients);
    if (read_pct > 0.0) {
        const double read_ratio = read_pct / 100.0;
        nupdates = llround(double(ngets_total) * (1.0 - read_ratio) / read_ratio);
        if (nupdates == 0) nupdates = 1;
    } else {
        nupdates = nsets;
    }
    logger = fopen(output_file.c_str(), "a");
    fprintf(
        logger,
        "client setting ngroups=%d, nclients=%d, nsets=%d, nupdates=%lu, ngets=%d, read_pct=%.3f, rps=%d\n",
        ngroups, nclients, nsets, nupdates, ngets, read_pct, rps);
    init_array();
    init_rng();
    prepare_key();
    run_set<kCreated>();  // set
    run_update();         // update
    run_get();            // get
    fclose(logger);
    return 0;
}
